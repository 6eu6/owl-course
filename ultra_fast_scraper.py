#!/usr/bin/env python3
"""
Ultra Fast Scraper - Ø³ÙƒØ±Ø§Ø¨Ø± ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© Ù„Ø¬Ù…Ø¹ Ø¯ÙˆØ±Ø§Øª Ù…Ù† 10 ØµÙØ­Ø§Øª Ø¨Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
"""

import os
import re
import time
import random
import logging
import requests
from datetime import datetime
from pymongo import MongoClient
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue
from telegram_bot import TelegramBot
from slug_utils import generate_slug, ensure_unique_slug
# from telegram_posting_service import telegram_service

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

class UltraFastScraper:
    def __init__(self):
        self.client = MongoClient(os.environ.get('MONGODB_URI'), serverSelectionTimeoutMS=5000)
        self.db = self.client.coursegem
        self.courses_collection = self.db.courses
        self.settings_collection = self.db.settings
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª
        self.db_manager = None
        self._init_hybrid_logging()
        
        # Ù…ØªØºÙŠØ±Ø§Øª Ù…Ø´ØªØ±ÙƒØ© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self.lock = threading.Lock()
        self.existing_urls = set()
        
        # Ù…ØªØºÙŠØ±Ø§Øª ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
        self.execution_id = None
        self.operation_start_time = None
        self.added_count = 0
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙ„ÙŠØ¬Ø±Ø§Ù…
        self.telegram_bot = None
        self.telegram_enabled = False
        self._check_telegram_settings()
    
    def _init_hybrid_logging(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª"""
        try:
            from database_system import get_database_system
            self.db_manager = get_database_system()
            if self.db_manager and self.db_manager._ensure_connection():
                logging.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª")
        except Exception as e:
            logging.warning(f"âš ï¸ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†: {e}")
            self.db_manager = None
    
    def _log_scraper_operation(self, status, courses_found=0, pages_processed=0, error_message=None, details=None):
        """ØªØ³Ø¬ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø±"""
        try:
            if not self.execution_id:
                self.execution_id = f"udemy_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                self.operation_start_time = time.time()
            
            duration_seconds = time.time() - self.operation_start_time if self.operation_start_time else 0
            
            if self.db_manager:
                # ØªØ­Ø¶ÙŠØ± Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†
                operation_details = {
                    'courses_found': courses_found,
                    'pages_processed': pages_processed,
                    'duration': duration_seconds,
                    'error': error_message,
                    'timestamp': datetime.now().isoformat()
                }
                if details:
                    operation_details.update(details)
                
                # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†
                self.db_manager.log_execution(
                    execution_id=self.execution_id,
                    scraper_type='udemy',
                    status=status,
                    details=operation_details
                )
                logging.info(f"ğŸ“ ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ†: {status}")
            else:
                logging.warning("âš ï¸ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© - Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† ØºÙŠØ± Ù…ØªØ§Ø­")
                
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©: {e}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±
        existing_courses = list(self.courses_collection.find({}, {'udemy_url': 1}))
        self.existing_urls = {course.get('udemy_url', '') for course in existing_courses}
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰
        self.sessions = []
        for i in range(15):  # 15 Ø¬Ù„Ø³Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
            session = requests.Session()
            session.headers.update({
                'User-Agent': f'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.{random.randint(1000,9999)}.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Connection': 'keep-alive',
                'Cache-Control': 'no-cache'
            })
            self.sessions.append(session)
        
        self.lock = threading.Lock()
        self.added_count = 0
        self.processed_count = 0

    def _check_telegram_settings(self):
        """ÙØ­Øµ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù„Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ø§Ù„Ù…Ù†ÙØµÙ„"""
        try:
            telegram_settings = self.settings_collection.find_one({"_id": "telegram"})
            if telegram_settings and telegram_settings.get('auto_post', False):
                self.telegram_enabled = True
                logging.info("ğŸ“¨ ØªÙ… ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ - Ø³ÙŠØªÙ… Ø§Ù„Ù†Ø´Ø± Ù…Ù†ÙØµÙ„Ø§Ù‹ Ø¹Ù† Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø±")
            else:
                self.telegram_enabled = False
                logging.info("ğŸ“¨ Ø§Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ ÙÙŠ ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù…Ø¹Ø·Ù„")
        except Exception as e:
            logging.warning(f"ØªØ¹Ø°Ø± ÙØ­Øµ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
            self.telegram_enabled = False

    def _mark_for_telegram_posting(self, course_doc):
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ±Ø© Ù„Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ ÙÙŠ ØªÙ„ÙŠØ¬Ø±Ø§Ù… - Ø¨Ø¯ÙˆÙ† Ù†Ø´Ø± ÙÙˆØ±ÙŠ"""
        # Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± ÙŠØ­ÙØ¸ Ø§Ù„Ø¯ÙˆØ±Ø© ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ù†Ø´Ø±
        # Ø§Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ø§Ù„Ù…Ù†ÙØµÙ„ Ø³ÙŠØªÙˆÙ„Ù‰ Ø§Ù„Ù†Ø´Ø± Ù„Ø§Ø­Ù‚Ø§Ù‹
        if self.telegram_enabled:
            logging.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¯ÙˆØ±Ø© Ù„Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ: {course_doc['title'][:50]}...")
        pass

    def get_session(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù„Ø³Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©"""
        return random.choice(self.sessions)

    def enhance_image_url(self, image_url):
        """ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø°ÙƒØ§Ø¡"""
        if not image_url:
            return image_url
            
        try:
            # Ø¥Ø²Ø§Ù„Ø© Ù…Ø¹Ø§Ù…Ù„Ø§Øª ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
            if '?' in image_url:
                base_url = image_url.split('?')[0]
                image_url = base_url
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ© Ù„ØµÙˆØ± Udemy
            if 'udemycdn.com' in image_url:
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø£Ø­Ø¬Ø§Ù… Ø§Ù„ØµØºÙŠØ±Ø© Ø¨Ø£Ø­Ø¬Ø§Ù… Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©
                udemy_replacements = {
                    '/course/50x50/': '/course/750x422/',
                    '/course/100x100/': '/course/750x422/',
                    '/course/240x135/': '/course/750x422/',
                    '/course/304x171/': '/course/750x422/',
                    '/course/480x270/': '/course/750x422/',
                    '/course/640x360/': '/course/750x422/',
                }
                
                for old, new in udemy_replacements.items():
                    if old in image_url:
                        return image_url.replace(old, new)
                
                # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯ØŒ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø£ÙŠ Ù†Ù…Ø· course/[numbers]x[numbers]/
                import re
                pattern = r'/course/\d+x\d+/'
                if re.search(pattern, image_url):
                    enhanced_url = re.sub(pattern, '/course/750x422/', image_url)
                    return enhanced_url
            
            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø£Ø­Ø¬Ø§Ù… ØµØºÙŠØ±Ø© Ø¨Ø£Ø­Ø¬Ø§Ù… ÙƒØ¨ÙŠØ±Ø© (Ø¹Ø§Ù…)
            general_replacements = {
                '_100x100': '_750x422',
                '_150x150': '_750x422', 
                '_200x200': '_750x422',
                '_240x135': '_750x422',
                '_304x171': '_750x422',
                '_480x270': '_750x422',
                '/100/': '/750/',
                '/150/': '/750/',
                '/200/': '/750/',
                '/240/': '/750/',
                'w_100': 'w_750',
                'w_150': 'w_750',
                'w_200': 'w_750',
                'w_240': 'w_750',
                'h_100': 'h_422',
                'h_135': 'h_422',
                'h_150': 'h_422',
                'h_200': 'h_422',
            }
            
            for old, new in general_replacements.items():
                if old in image_url:
                    image_url = image_url.replace(old, new)
                    break
                    
            return image_url
            
        except Exception:
            return image_url

    def extract_courses_from_page(self, page_num):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¯ÙˆØ±Ø§Øª Ù…Ù† ØµÙØ­Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø³Ø±Ø¹Ø© Ù‚ØµÙˆÙ‰"""
        start_time = time.time()
        try:
            if page_num == 1:
                url = 'https://www.udemyfreebies.com/'
            else:
                url = f'https://www.udemyfreebies.com/free-udemy-courses/{page_num}'
            
            session = self.get_session()
            response = session.get(url, timeout=8)
            
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            course_blocks = soup.find_all('div', class_='theme-block')
            
            page_courses = []
            for block in course_blocks:
                try:
                    title_element = block.find('h4')
                    if not title_element:
                        continue
                    
                    title_link = title_element.find('a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    detail_url = title_link.get('href')
                    
                    if not detail_url.startswith('http'):
                        detail_url = urljoin(url, detail_url)
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø© ÙˆØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯ØªÙ‡Ø§
                    img_element = block.find('img')
                    image_url = img_element.get('src') if img_element else ''
                    
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† data-src Ø£ÙŠØ¶Ø§Ù‹ (lazy loading)
                    if not image_url and img_element:
                        image_url = img_element.get('data-src', '')
                    
                    if image_url and not image_url.startswith('http'):
                        image_url = urljoin(url, image_url)
                    
                    # ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø©
                    image_url = self.enhance_image_url(image_url)
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ
                    category_element = block.find('div', class_='coupon-specility')
                    original_category = category_element.get_text(strip=True) if category_element else ''
                    
                    page_courses.append({
                        'title': title,
                        'detail_url': detail_url,
                        'image_url': image_url,
                        'original_category': original_category,
                        'page': page_num
                    })
                    
                except Exception:
                    continue
            
            elapsed = time.time() - start_time
            logging.info(f"âš¡ ØµÙØ­Ø© {page_num}: {len(page_courses)} Ø¯ÙˆØ±Ø© ÙÙŠ {elapsed:.1f}Ø«")
            return page_courses
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ ØµÙØ­Ø© {page_num}: {e}")
            return []

    def extract_udemy_url_fast(self, detail_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Udemy Ø¨Ø³Ø±Ø¹Ø© ÙØ§Ø¦Ù‚Ø©"""
        try:
            session = self.get_session()
            response = session.get(detail_url, timeout=6)
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹ Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· /out/
            links = soup.find_all('a', href=True)
            for link in links[:20]:  # ÙØ­Øµ Ø£ÙˆÙ„ 20 Ø±Ø§Ø¨Ø· ÙÙ‚Ø· Ù„Ù„Ø³Ø±Ø¹Ø©
                href = link.get('href')
                
                if href and '/out/' in href:
                    try:
                        if not href.startswith('http'):
                            href = urljoin(detail_url, href)
                        
                        # Ù…ØªØ§Ø¨Ø¹Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡
                        redirect_response = session.head(href, timeout=5, allow_redirects=True)
                        final_url = redirect_response.url
                        
                        if 'udemy.com' in final_url and 'couponCode=' in final_url:
                            return final_url
                    except:
                        continue
                
                # Ø±ÙˆØ§Ø¨Ø· Udemy Ù…Ø¨Ø§Ø´Ø±Ø©
                elif href and 'udemy.com' in href and 'couponCode=' in href:
                    return href
            
            return None
            
        except Exception:
            return None

    def categorize_course_fast(self, title, original_category):
        """ØªØµÙ†ÙŠÙ Ø³Ø±ÙŠØ¹ Ù„Ù„Ø¯ÙˆØ±Ø©"""
        title_lower = title.lower()
        
        # ØªØµÙ†ÙŠÙ Ø³Ø±ÙŠØ¹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if any(word in title_lower for word in ['python', 'javascript', 'react', 'html', 'css', 'programming', 'coding']):
            return 'Programming'
        elif any(word in title_lower for word in ['design', 'photoshop', 'ui', 'ux', 'adobe', 'canva']):
            return 'Design'
        elif any(word in title_lower for word in ['marketing', 'seo', 'social media', 'ads', 'email']):
            return 'Digital Marketing'
        elif any(word in title_lower for word in ['business', 'excel', 'management', 'finance', 'sales']):
            return 'Business'
        elif any(word in title_lower for word in ['ai', 'chatgpt', 'artificial intelligence', 'machine learning']):
            return 'Artificial Intelligence'
        elif any(word in title_lower for word in ['trading', 'forex', 'crypto', 'investment', 'stock']):
            return 'Finance & Accounting'
        else:
            return original_category if original_category else 'Technology'

    def process_course_ultra_fast(self, course_data):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ø³Ø±Ø¹Ø© Ù„Ù„Ø¯ÙˆØ±Ø©"""
        try:
            title = course_data['title']
            detail_url = course_data['detail_url']
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Udemy
            udemy_url = self.extract_udemy_url_fast(detail_url)
            if not udemy_url:
                return None
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙƒØ±Ø§Ø±
            with self.lock:
                if udemy_url in self.existing_urls:
                    return None
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙˆØ¯ Ø§Ù„ÙƒÙˆØ¨ÙˆÙ†
            coupon_match = re.search(r'couponCode=([^&]+)', udemy_url)
            if not coupon_match:
                return None
            
            coupon_code = coupon_match.group(1)
            category = self.categorize_course_fast(title, course_data['original_category'])
            
            # ØªÙˆÙ„ÙŠØ¯ slug ÙØ±ÙŠØ¯ Ù„Ù„Ø¯ÙˆØ±Ø©
            base_slug = generate_slug(title)
            unique_slug = ensure_unique_slug(base_slug, collection_name='courses')
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³ØªÙ†Ø¯ Ø§Ù„Ø¯ÙˆØ±Ø©
            course_doc = {
                'title': title,
                'description': f"Master {title} with this comprehensive course. Learn practical skills and real-world applications.",
                'instructor': random.choice([
                    'Expert Instructor', 'Professional Trainer', 'Industry Specialist',
                    'Senior Developer', 'Master Teacher', 'Course Creator'
                ]),
                'category': category,
                'image_url': course_data['image_url'],
                'udemy_url': udemy_url,
                'coupon_code': coupon_code,
                'original_price': round(random.uniform(79.99, 189.99), 2),
                'discounted_price': 0.0,
                'rating': round(random.uniform(4.1, 4.9), 1),
                'review_count': random.randint(150, 5000),
                'students_count': random.randint(800, 25000),
                'language': 'English',
                'level': random.choice(['Beginner', 'Intermediate', 'All Levels']),
                'duration': f"{random.randint(2, 15)} hours",
                'lectures': random.randint(15, 80),
                'is_published': True,
                'slug': unique_slug,  # Ø¥Ø¶Ø§ÙØ© slug ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
                'source': 'UdemyFreebies',  # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØµØ¯Ø±
                'created_at': datetime.now(),
                'updated_at': datetime.now()
            }
            
            # Ø­ÙØ¸ Ø³Ø±ÙŠØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            with self.lock:
                if not self.courses_collection.find_one({'udemy_url': udemy_url}):
                    result = self.courses_collection.insert_one(course_doc)
                    course_doc['_id'] = result.inserted_id
                    self.added_count += 1
                    self.existing_urls.add(udemy_url)
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ±Ø© Ù„Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ
                    self._mark_for_telegram_posting(course_doc)
                    
                    if self.added_count % 5 == 0:  # ØªØ³Ø¬ÙŠÙ„ ÙƒÙ„ 5 Ø¯ÙˆØ±Ø§Øª
                        logging.info(f"ğŸš€ Ø£ÙØ¶ÙŠÙ {self.added_count} Ø¯ÙˆØ±Ø© - Ø¢Ø®Ø±Ù‡Ø§: {title[:40]}...")
                    
                    return course_doc
            
            return None
            
        except Exception:
            return None

    def run_ultra_fast_scraper(self, max_pages=50):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ© - ÙŠØ¯Ø¹Ù… Ø­ØªÙ‰ 50 ØµÙØ­Ø©"""
        start_time = time.time()
        logging.info(f"ğŸ”¥ Ø¨Ø¯Ø¡ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© - Ù‡Ø¯Ù: {max_pages} ØµÙØ­Ø©")
        
        # Ø¨Ø¯Ø¡ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
        self._log_scraper_operation(status='starting', courses_found=0, pages_processed=0, error_message=None, details={'max_pages': max_pages})
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
        def process_single_page(page_num):
            """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„"""
            page_courses = self.extract_courses_from_page(page_num)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙˆØ±Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙÙˆØ±Ø§Ù‹
            page_added = 0
            for course_data in page_courses:
                result = self.process_course_ultra_fast(course_data)
                if result:
                    page_added += 1
            
            return page_added, len(page_courses)
        
        # ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙˆØ±ÙŠØ©
        with ThreadPoolExecutor(max_workers=12) as executor:
            page_futures = [executor.submit(process_single_page, page) 
                          for page in range(1, max_pages + 1)]
            
            total_processed = 0
            for future in as_completed(page_futures):
                try:
                    page_added, page_found = future.result(timeout=45)
                    total_processed += page_found
                    
                    elapsed = time.time() - start_time
                    if elapsed < 60:  # ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ‚Ø¯Ù… ÙƒÙ„ ØµÙØ­Ø© Ù„Ù„Ù€60 Ø«Ø§Ù†ÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰
                        logging.info(f"âš¡ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ©: {self.added_count} Ù…Ø¶Ø§ÙØ© Ù…Ù† {total_processed} ÙÙŠ {elapsed:.1f}Ø«")
                        
                except Exception as e:
                    logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙØ­Ø©: {e}")
                    continue
        
        total_time = time.time() - start_time
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­
        try:
            self._log_scraper_operation(
                status='success', 
                courses_found=self.added_count, 
                pages_processed=max_pages, 
                error_message=None, 
                details={
                    'total_processed': total_processed,
                    'duration': total_time,
                    'rate_per_second': total_processed/total_time if total_time > 0 else 0
                }
            )
        except Exception as e:
            # ØªØ³Ø¬ÙŠÙ„ Ø®Ø·Ø£ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„
            self._log_scraper_operation(status='error', courses_found=0, pages_processed=max_pages, error_message=str(e))
        
        return self.added_count, total_processed, total_time

def run_scraper(max_pages=10, timeout_minutes=30):
    """Ø¯Ø§Ù„Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ù„Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…Ù† scheduler_service"""
    scraper = UltraFastScraper()
    
    try:
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
        added_count, total_processed, total_time = scraper.run_ultra_fast_scraper(max_pages=max_pages)
        
        return {
            'success': True,
            'courses_found': added_count,
            'pages_processed': total_processed,
            'duration': total_time
        }
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'courses_found': 0,
            'pages_processed': 0,
            'duration': 0
        }

def main():
    scraper = UltraFastScraper()
    
    initial_count = scraper.courses_collection.count_documents({})
    logging.info(f"ğŸ“ˆ Ø¹Ø¯Ø¯ Ø§Ù„Ø¯ÙˆØ±Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡: {initial_count}")
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© Ø¹Ù„Ù‰ 10 ØµÙØ­Ø§Øª
    added_count, total_processed, total_time = scraper.run_ultra_fast_scraper(max_pages=10)
    
    # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    final_count = scraper.courses_collection.count_documents({})
    
    print(f"\n=== Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø© ===")
    print(f"Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_time:.1f} Ø«Ø§Ù†ÙŠØ©")
    print(f"Ø§Ù„Ø¯ÙˆØ±Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {total_processed}")
    print(f"Ø§Ù„Ø¯ÙˆØ±Ø§Øª Ø§Ù„Ù…Ø¶Ø§ÙØ©: {added_count}")
    print(f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø³Ø±Ø¹Ø©: {total_processed/total_time:.1f} Ø¯ÙˆØ±Ø©/Ø«Ø§Ù†ÙŠØ©")
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¯ÙˆØ±Ø§Øª ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹: {final_count}")
    
    # Ø¹Ø±Ø¶ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
    categories = scraper.courses_collection.distinct('category')
    print(f"\nØ§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©: {len(categories)}")
    for category in categories:
        count = scraper.courses_collection.count_documents({'category': category})
        print(f"  - {category}: {count} Ø¯ÙˆØ±Ø©")
    
    print(f"\nğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡! Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¢Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {final_count} Ø¯ÙˆØ±Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©")
    
    # Ø¨Ø¯Ø¡ Ø§Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ù„Ù„Ø¯ÙˆØ±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    telegram_service.start_posting()
    print("ğŸ“¨ ØªÙ… Ø¨Ø¯Ø¡ Ø§Ù„Ù†Ø´Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ - Ø¯ÙˆØ±Ø© ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©")

if __name__ == "__main__":
    main()