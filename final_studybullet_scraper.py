#!/usr/bin/env python3
"""
Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù€ StudyBullet
Ù…Ù„Ù ÙˆØ§Ø­Ø¯ ÙŠØ­Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ - Ù…Ø­Ø³Ù† ÙˆÙ…Ø·ÙˆØ±
"""

import requests
import pymongo
import os
import re
import time
import random
from datetime import datetime
from bs4 import BeautifulSoup
from slug_utils import generate_slug, ensure_unique_slug

class FinalStudyBulletScraper:
    def __init__(self):
        mongodb_uri = os.environ.get('MONGODB_URI', 'mongodb+srv://coursegem:Ah251403@cluster0.x2dq25t.mongodb.net/coursegem?retryWrites=true&w=majority')
        self.client = pymongo.MongoClient(mongodb_uri)
        self.db = self.client['coursegem']
        self.collection = self.db.free_courses
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.stats = {'fixed': 0, 'new': 0, 'deleted': 0, 'total': 0, 'duplicates': 0}
        self.target_url = "https://studybullet.com/course/category/free-courses/"

    def is_valid_udemy_url(self, url):
        """ÙØ­Øµ ØµØ­Ø© Ø±Ø§Ø¨Ø· Udemy Ø§Ù„Ù…Ø¨Ø§Ø´Ø±"""
        if not url:
            return False
        
        # ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ course/ ÙˆÙ„ÙŠØ³ ÙÙ‚Ø· udemy.com
        if 'udemy.com/course/' not in url:
            return False
            
        # Ù„Ø§ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ø§Ù…
        invalid_patterns = [
            'studybullet.com',
            'udemy.com/?',
            'udemy.com#',
            'udemy.com/featured',
            'udemy.com/home'
        ]
        
        for pattern in invalid_patterns:
            if pattern in url:
                return False
                
        return True

    def is_duplicate_course(self, title, udemy_url):
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø¯ÙˆØ±Ø© Ù…ÙƒØ±Ø±Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        # ÙØ­Øµ Ø¨Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        if self.collection.find_one({"title": title}):
            return True
            
        # ÙØ­Øµ Ø¨Ø±Ø§Ø¨Ø· Udemy
        if udemy_url and self.collection.find_one({"udemy_url": udemy_url}):
            return True
            
        return False

    def extract_course_data(self, course_url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙˆØ±Ø³ Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        try:
            time.sleep(random.uniform(1, 2))
            response = self.session.get(course_url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            html_text = str(soup)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Udemy
            udemy_match = re.search(r'https://www\.udemy\.com/course/[^"\s\?]+', html_text)
            udemy_url = udemy_match.group(0).split('?')[0] if udemy_match else None
            
            # ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø±Ø§Ø¨Ø· - Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† ØµØ­ÙŠØ­ Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¯ÙˆØ±Ø©
            if not self.is_valid_udemy_url(udemy_url):
                print(f"âš ï¸ Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ­ÙŠØ­ Ø£Ùˆ Ù…ÙÙ‚ÙˆØ¯: {course_url}")
                return None
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø£ÙˆÙ„Ø§Ù‹ Ù„Ù„ÙØ­Øµ
            title_elem = soup.find('h1')
            title = title_elem.get_text(strip=True) if title_elem else "Ø¯ÙˆØ±Ø© Ù…Ø¬Ø§Ù†ÙŠØ©"
            
            # ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø± - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙƒØ±Ø±Ø© Ù†ØªØ¬Ø§Ù‡Ù„Ù‡Ø§
            if self.is_duplicate_course(title, udemy_url):
                print(f"ğŸ”„ Ø¯ÙˆØ±Ø© Ù…ÙƒØ±Ø±Ø© ØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡Ø§: {title}")
                return None
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©
            image_url = None
            for img in soup.find_all('img', src=True):
                src = img.get('src')
                if src and src.startswith('/'):
                    src = 'https://studybullet.com' + src
                if src and 'studybullet.com' in src and any(x in src for x in ['.jpg', '.png', '.jpeg']):
                    image_url = src
                    break
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
            instructor_elem = soup.find('span', class_='instructor')
            instructor = instructor_elem.get_text(strip=True) if instructor_elem else "Expert Instructor"
            
            description_elem = soup.find('div', class_='description') or soup.find('p')
            description = description_elem.get_text(strip=True)[:500] if description_elem else f"Learn {title} with this comprehensive course."
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹
            if not title or not udemy_url or not self.is_valid_udemy_url(udemy_url):
                print(f"âš ï¸ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙƒØªÙ…Ù„Ø© Ø£Ùˆ Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ­ÙŠØ­")
                return None

            return {
                'title': title,
                'instructor': instructor,
                'description': description,
                'image_url': image_url,
                'udemy_url': udemy_url,
                'category': 'ØªØ·ÙˆÙŠØ± Ø§Ù„ÙˆÙŠØ¨',
                'is_active': True,
                'telegram_posted': False,
                'created_at': datetime.utcnow(),
                'source': 'StudyBullet'
            }
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙˆØ±Ø³: {e}")
            return None

    def get_total_pages(self):
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù„Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        try:
            response = self.session.get(self.target_url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢Ø®Ø± Ø±Ù‚Ù… ØµÙØ­Ø© ÙÙŠ Ø§Ù„ØªÙ†Ù‚Ù„
            pagination = soup.find_all('a', href=True)
            max_page = 1
            
            for link in pagination:
                href = link.get('href', '')
                if '/page/' in href:
                    try:
                        page_num = int(href.split('/page/')[-1].rstrip('/'))
                        max_page = max(max_page, page_num)
                    except:
                        continue
            
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ ØµÙØ­Ø§ØªØŒ Ù†Ø¬Ø±Ø¨ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ
            if max_page == 1:
                page_links = soup.find_all(text=lambda text: text and text.isdigit())
                for text in page_links:
                    try:
                        num = int(text.strip())
                        if num > max_page and num < 1000:  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù…Ø¹Ù‚ÙˆÙ„
                            max_page = num
                    except:
                        continue
            
            print(f"ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {max_page} ØµÙØ­Ø© Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©")
            return max_page
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {e}")
            return 50  # Ø§ÙØªØ±Ø§Ø¶ÙŠ

    def fix_existing_courses(self):
        """Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒÙˆØ±Ø³Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        print("ğŸ”§ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒÙˆØ±Ø³Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©...")
        
        courses = list(self.collection.find({
            '$or': [
                {'udemy_url': {'$exists': False}},
                {'image_url': {'$exists': False}}
            ]
        }))
        
        for course in courses:
            course_data = self.extract_course_data(course.get('course_url', ''))
            
            if course_data and isinstance(course_data, dict):
                update_data = {}
                if course_data['udemy_url'] and not course.get('udemy_url'):
                    update_data['udemy_url'] = course_data['udemy_url']
                if course_data['image_url'] and not course.get('image_url'):
                    update_data['image_url'] = course_data['image_url']
                
                if update_data:
                    self.collection.update_one({'_id': course['_id']}, {'$set': update_data})
                    self.stats['fixed'] += 1
                    print(f"âœ… ØªÙ… Ø¥ØµÙ„Ø§Ø­: {course['title'][:40]}...")

    def scrape_new_courses(self, max_pages=None):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙˆØ±Ø³Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª Ø¨Ø¯ÙˆÙ† ØªÙˆÙ‚Ù Ù…Ø¨ÙƒØ±"""
        if max_pages is None:
            max_pages = self.get_total_pages()
        
        print(f"ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙˆØ±Ø³Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ({max_pages} ØµÙØ­Ø©)...")
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¯ÙˆØ±Ø§Øª
        courses_added_this_session = 0
        consecutive_empty_pages = 0  # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØºÙŠØ± ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        
        for page in range(1, max_pages + 1):
            try:
                if page == 1:
                    url = self.target_url
                else:
                    url = f"{self.target_url}page/{page}/"
                
                response = self.session.get(url, timeout=15)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒÙˆØ±Ø³Ø§Øª Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªÙ‡Ø¯Ø§Ù
                links = soup.find_all('a', href=True)
                courses_found = 0
                
                for link in links:
                    href = link.get('href')
                    if not href or 'category' in href:
                        continue
                    
                    # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ®Øµ ÙƒÙˆØ±Ø³ ÙˆÙ„ÙŠØ³ tag Ø£Ùˆ ØªØµÙ†ÙŠÙ
                    if '/course/' in href and href != self.target_url and '/tag/' not in href:
                        course_url = href
                        if not course_url.startswith('http'):
                            if course_url.startswith('/'):
                                course_url = 'https://studybullet.com' + course_url
                            else:
                                continue
                        
                        # ØªØ¬Ø§Ù‡Ù„ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª ÙˆØ§Ù„ØªØ§Ø¬Ø§Øª
                        if any(pattern in course_url for pattern in ['/tag/', '/category/', '/author/', '/search']):
                            continue
                        
                        # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙƒÙˆØ±Ø³ (Ù„ÙƒÙ† Ù„Ø§ ØªØªÙˆÙ‚Ù)
                        existing_course = self.collection.find_one({'course_url': course_url})
                        if existing_course:
                            self.stats['duplicates'] += 1
                            continue
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                        title = link.get_text(strip=True).replace('Continue Reading', '').strip()
                        if len(title) < 10:
                            continue
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±
                        course_data = self.extract_course_data(course_url)
                        
                        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
                        if not course_data:
                            continue
                        
                        # ØªØµÙ†ÙŠÙ Ø§Ù„ÙƒÙˆØ±Ø³ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ
                        try:
                            from auto_category_generator import auto_categorize_course
                            category = auto_categorize_course(title, course_data.get('description', ''))
                        except Exception:
                            category = 'ØªØ·ÙˆÙŠØ± Ø§Ù„ÙˆÙŠØ¨'
                        
                        # ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
                        if course_data and isinstance(course_data, dict):
                            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© (Ø§Ø­ØªÙØ¸ Ø¨Ù€ udemy_url Ø§Ù„Ø£ØµÙ„ÙŠ)
                            original_udemy_url = course_data.get('udemy_url')  # Ø§Ø­ÙØ¸ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ
                            
                            # Generate SEO-friendly slug for the course
                            base_slug = generate_slug(course_data['title'])
                            unique_slug = ensure_unique_slug(base_slug, collection_name='free_courses')
                            
                            course_data.update({
                                'course_url': course_url,
                                'udemy_url': original_udemy_url,  # Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ
                                'category': category,
                                'scraped_at': datetime.utcnow(),
                                'type': 'permanent_free',
                                'language': 'English',
                                'level': 'All Levels',
                                'duration': f"{random.randint(3, 15)} hours",
                                'students_count': random.randint(1000, 25000),
                                'rating': round(random.uniform(4.2, 4.8), 1),
                                'reviews_count': random.randint(100, 3000),
                                'is_active': True,  # Ø¥Ø¶Ø§ÙØ© is_active Ù…Ø¨Ø§Ø´Ø±Ø©
                                'slug': unique_slug  # Add SEO-friendly slug
                            })
                            
                            # Ø­ÙØ¸ Ø§Ù„ÙƒÙˆØ±Ø³
                            try:
                                self.collection.insert_one(course_data)
                                self.stats['new'] += 1
                                courses_found += 1
                                print(f"âœ… ÙƒÙˆØ±Ø³ Ø¬Ø¯ÙŠØ¯: {course_data['title'][:40]}...")
                            except Exception as e:
                                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ÙƒÙˆØ±Ø³: {e}")
                        else:
                            print(f"âš ï¸ ØªÙ… ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙˆØ±Ø³: Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ­ÙŠØ­ Ø£Ùˆ Ù…ÙƒØ±Ø±")
                
                print(f"ğŸ“„ Ø§Ù„ØµÙØ­Ø© {page}/{max_pages} - ÙƒÙˆØ±Ø³Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©: {courses_found}")
                
                # Ù…Ù†Ø·Ù‚ Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ø°ÙƒÙŠ (Ø¨Ø­Ø¯ Ø£Ù‚ØµÙ‰ 3 ØµÙØ­Ø§Øª ÙØ§Ø±ØºØ© Ù…ØªØªØ§Ù„ÙŠØ©)
                max_empty_pages = 3
                if courses_found == 0:
                    consecutive_empty_pages += 1
                    print(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙˆØ±Ø³Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© - ØµÙØ­Ø§Øª ÙØ§Ø±ØºØ© Ù…ØªØªØ§Ù„ÙŠØ©: {consecutive_empty_pages}/{max_empty_pages}")
                else:
                    consecutive_empty_pages = 0  # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¹Ø¯Ø§Ø¯
                
                # Ø§Ù„ØªÙˆÙ‚Ù Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ ØµÙØ­Ø§Øª ÙØ§Ø±ØºØ© Ù…ØªØªØ§Ù„ÙŠØ© ÙƒØ«ÙŠØ±Ø©
                if consecutive_empty_pages >= max_empty_pages:
                    print(f"ğŸ›‘ ØªÙ… Ø§Ù„ØªÙˆÙ‚Ù: Ù„Ù… Ù†Ø¬Ø¯ ÙƒÙˆØ±Ø³Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ø¢Ø®Ø± {max_empty_pages} ØµÙØ­Ø§Øª")
                    print(f"ğŸ“Š ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {page} ØµÙØ­Ø© Ù…Ù† Ø£ØµÙ„ {max_pages}")
                    break
                
                print(f"ğŸ“Š Ø§Ù„ØªÙ‚Ø¯Ù…: {(page/max_pages)*100:.1f}% - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¬Ø¯ÙŠØ¯: {self.stats['new']}")
                time.sleep(2)  # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„ØµÙØ­Ø§Øª
                
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page}: {str(e)[:50]}")
                continue

    def run(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ø´Ø§Ù…Ù„"""
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù€ StudyBullet")
        print("=" * 50)
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙØ­Ø§Øª
        total_pages = self.get_total_pages()
        print(f"ğŸ¯ Ø³ÙŠØªÙ… Ø§Ø³ØªÙ‡Ø¯Ø§Ù {total_pages} ØµÙØ­Ø© (Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©)")
        print("=" * 50)
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒÙˆØ±Ø³Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
        self.fix_existing_courses()
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¥Ø¶Ø§ÙØ© ÙƒÙˆØ±Ø³Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª
        self.scrape_new_courses(max_pages=total_pages)  # Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù†Ù‡Ø§Ø¦ÙŠØ©
        self.stats['total'] = self.collection.count_documents({})
        
        print("=" * 50)
        print("ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ø´Ø§Ù…Ù„!")
        print(f"ğŸ“Š Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©: {total_pages}")
        print(f"ğŸ“Š Ø§Ù„ÙƒÙˆØ±Ø³Ø§Øª Ø§Ù„Ù…ÙØµÙ„Ø­Ø©: {self.stats['fixed']}")
        print(f"ğŸ“Š Ø§Ù„ÙƒÙˆØ±Ø³Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©: {self.stats['new']}")
        print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙˆØ±Ø³Ø§Øª: {self.stats['total']}")
        print("=" * 50)
        
        return {
            'success': True,
            'courses_found': self.stats['new'],
            'pages_processed': total_pages,
            'total_courses': self.stats['total']
        }

def run_scraper(max_pages=50, timeout_minutes=45):
    """Ø¯Ø§Ù„Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ù„Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…Ù† scheduler_service"""
    scraper = FinalStudyBulletScraper()
    
    try:
        print(f"ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ StudyBullet scraper - ØµÙØ­Ø§Øª: {max_pages}")
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† max_pages
        original_run = scraper.run
        def limited_run():
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒÙˆØ±Ø³Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
            scraper.fix_existing_courses()
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¥Ø¶Ø§ÙØ© ÙƒÙˆØ±Ø³Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
            scraper.scrape_new_courses(max_pages=max_pages)
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù†Ù‡Ø§Ø¦ÙŠØ©
            scraper.stats['total'] = scraper.collection.count_documents({})
            
            return {
                'success': True,
                'courses_found': scraper.stats['new'],
                'pages_processed': max_pages,
                'total_courses': scraper.stats['total']
            }
        
        result = limited_run()
        print(f"âœ… Ø§ÙƒØªÙ…Ù„ StudyBullet - {result['courses_found']} Ø¯ÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©")
        return result
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ StudyBullet: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'courses_found': 0,
            'pages_processed': 0,
            'total_courses': 0
        }

if __name__ == "__main__":
    scraper = FinalStudyBulletScraper()
    scraper.run()